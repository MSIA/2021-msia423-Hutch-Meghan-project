{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e191988",
   "metadata": {},
   "source": [
    "# Process tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4096c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.test.utils import common_corpus, common_dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "#import pyLDAvis\n",
    "#import pyLDAvis.sklearn\n",
    "#import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c187a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data = pd.read_csv('../data/external/constructs.csv')\n",
    "tweet_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725a8af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweet_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b9c88f",
   "metadata": {},
   "source": [
    "## Format Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b09bd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data['date'] = tweet_data['created_at'].str.split(' ').str[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516a32fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data['date'] = tweet_data['date'].str.join(' ')\n",
    "tweet_data['date'] = tweet_data['date'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e1ed6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data['date'] = pd.to_datetime(tweet_data['date'] + ' 2020', format='%b %d %Y', errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8194b18d",
   "metadata": {},
   "source": [
    "## Select Time Frame of Interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0686be9c",
   "metadata": {},
   "source": [
    "We should have two time frames and allow users to compare the differences in topics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaa1832",
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_input = pd.to_datetime('2020-01-01')\n",
    "\n",
    "timeframe1 = tweet_data[(tweet_data['date'] >= usr_input) & (tweet_data['date'] < (usr_input + timedelta(days=30)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cd3155",
   "metadata": {},
   "source": [
    "## Tokenize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3f103e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# additional characters to remove\n",
    "def number_list(r1, r2):\n",
    "    return list(range(r1, r2+1))\n",
    "\n",
    "alphabet_remove = list(string.ascii_lowercase)\n",
    "\n",
    "stop_words = stop_words.union(number_remove, alphabet_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f9dd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be31894a",
   "metadata": {},
   "source": [
    "Code adapted https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a44049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(tweets):\n",
    "    stop_free = \" \".join([i for i in tweets.lower().split() if i not in stop_words])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e5260b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timeit.timeit()\n",
    "\n",
    "doc_clean = [clean(tweets).split() for tweets in timeframe['read_text_clean2']] \n",
    "end = timeit.timeit()\n",
    "\n",
    "print((end - start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d47c34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e528e03b",
   "metadata": {},
   "source": [
    "## Train LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b609083",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for t in range(2, 20):\n",
    "    cov_model = LdaModel(corpus = doc_term_matrix, id2word = dictionary, num_topics = t, random_state=66826)\n",
    "\n",
    "    cm = CoherenceModel(model=cov_model, dictionary=dictionary, texts=doc_clean, coherence='c_v')\n",
    "    score = cm.get_coherence()\n",
    "    tup = t, score\n",
    "    results.append(tup)\n",
    "\n",
    "results = pd.DataFrame(results, columns=['topic', 'score'])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139387b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(results.score.values, index=results.topic.values)\n",
    "_ = s.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3714fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_model = LdaModel(corpus = doc_term_matrix, id2word = dictionary, num_topics = 14, random_state=66826)\n",
    "\n",
    "cm = CoherenceModel(model=cov_model, dictionary=dictionary, texts=doc_clean, coherence='c_v')\n",
    "coherence = cm.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddab0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coherence)\n",
    "cov_model.print_topics(num_topics=4, num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c28ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_model.show_topics(num_topics = -1, num_words=20, formatted=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c0d259",
   "metadata": {},
   "source": [
    "Get topics for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3540cab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topics = cov_model.get_document_topics(doc_term_matrix, minimum_probability=None, minimum_phi_value=None, per_word_topics=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baef5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_max = []\n",
    "\n",
    "for d in range(len(doc_topics)):\n",
    "    max_topic = max(doc_topics[d])\n",
    "    topic_df = pd.DataFrame(max_topic).transpose()\n",
    "    topic_df.columns = ['topic_num', 'prob']\n",
    "    timeframe_slice = timeframe[['read_text_clean2','Perceived_susceptibility', 'Perceived_severity', 'Perceived_benefits', 'Perceived_barriers']].iloc[[d]]\n",
    "    timeframe_slice = timeframe_slice.reset_index()\n",
    "    topic_df = pd.concat([topic_df, timeframe_slice], axis=1, join=\"inner\")\n",
    "    del topic_df['index'] \n",
    "    doc_topic_max.append(topic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943a1f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_max_df = pd.concat(doc_topic_max)\n",
    "doc_topic_max_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f285e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accb7b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_matrix = doc_topic_max_df.groupby(['topic_num'])['Perceived_susceptibility', 'Perceived_severity', 'Perceived_benefits', 'Perceived_barriers'].sum().reset_index()\n",
    "doc_topic_matrix['count'] = doc_topic_matrix['topic_num'].map(doc_topic_max_df['topic_num'].value_counts())\n",
    "doc_topic_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61aa6451",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(background_color='black',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=20,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a22c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709ca2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics[11:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8c6349",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(10,10), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[10:14][i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be4e10b",
   "metadata": {},
   "source": [
    "If I were to subset Topic 1, what are the most frequent health belief classifications?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3082f3",
   "metadata": {},
   "source": [
    "# Save Dataframe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
