{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972ca00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.process_data import load_tweet_data, remove_duplicates, format_dates, timeframe, clean_text, create_dictionary\n",
    "from src.train_lda import topic_eval, get_max_k, train_lda, get_doc_topic_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5558e028",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ccecfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim import corpora\n",
    "from gensim.test.utils import common_corpus, common_dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# create stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "alphabet_remove = list(string.ascii_lowercase)\n",
    "\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "number_remove = list(range(0, 9999))\n",
    "number_remove = map(str, number_remove) \n",
    "\n",
    "stop_words = stop_words.union(number_remove, alphabet_remove)\n",
    "\n",
    "# define lemmatizer object\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12679603",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b37062",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data = load_tweet_data(data_path = 'data/external/constructs.csv')\n",
    "#tweet_data = load_tweet_data(data_path = 'data/sample/tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404b69ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data = remove_duplicates(tweet_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00f4d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data_formatted = format_dates(tweet_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e683578c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data_formatted, input_date = timeframe(tweet_data_formatted, '2020-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095ce45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_clean = [clean_text(tweets, exclude, stop_words, lemma).split() for tweets in tweet_data_formatted['read_text_clean2']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70883447",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary, doc_term_matrix = create_dictionary(doc_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e132af",
   "metadata": {},
   "source": [
    "# Train LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701797ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_eval(doc_clean, doc_term_matrix, dictionary, top_k, input_date):\n",
    "    \"\"\"Evaluate the number of topics (k) to choose via the highest coherence score.\n",
    "    \n",
    "    Args: \n",
    "        doc_clean: dataframe - dataframe with processed text.\n",
    "        doc_term_matrix: list - bag of words matrix with frequency of each term mapped to dictionary id. \n",
    "        dictionary: corpora.dictionary - dictionary mapping each term to it's integer id.\n",
    "        top_k: int - max number of topics to test.\n",
    "        input_date: str - date that the user selected to subset the data.\n",
    "    \n",
    "    Return: \n",
    "        lda_results: dataframe - of scores from the k topic evaluation\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for t in range(4, top_k):\n",
    "        \n",
    "        cov_model = LdaModel(corpus = doc_term_matrix, id2word = dictionary, num_topics = t, random_state=66826)\n",
    "\n",
    "        cm = CoherenceModel(model=cov_model, dictionary=dictionary, texts=doc_clean, coherence='c_v')\n",
    "        score = cm.get_coherence()\n",
    "        tup = t, score\n",
    "        results.append(tup)\n",
    "\n",
    "    lda_results = pd.DataFrame(results, columns=['topic', 'score'])\n",
    "    \n",
    "    # save plots\n",
    "    s = pd.Series(lda_results.score.values, index=lda_results.topic.values)\n",
    "    \n",
    "    pltk = s.plot()\n",
    "    fig = pltk.get_figure()\n",
    "    fig.savefig(\"app/static\" + input_date + \".png\")\n",
    "    \n",
    "    return lda_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f568de99",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_results = topic_eval(doc_clean, doc_term_matrix, dictionary, top_k = 6, input_date = input_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527de025",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5272d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_k(lda_results):\n",
    "    \n",
    "    max_k = lda_results.iloc[lda_results['score'].argmax()][['topic']].astype(int)\n",
    "    \n",
    "    return max_k\n",
    "\n",
    "max_k = get_max_k(lda_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774ff3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lda(max_k):\n",
    "    \"\"\"Train the lda model on the max K found during topic evaluation\n",
    "    \n",
    "    Args: max_k: int - specifies the number of topics the model should generate.\n",
    "    \n",
    "    Return: \n",
    "        cov_model: trained lda model object.\n",
    "        coherence_score: int - return coherence score\n",
    "    \"\"\"\n",
    "    \n",
    "    cov_model = LdaModel(corpus = doc_term_matrix, id2word = dictionary, num_topics = max_k, random_state=66826)\n",
    "    cm = CoherenceModel(model=cov_model, dictionary=dictionary, texts=doc_clean, coherence='c_v')\n",
    "    coherence_score = cm.get_coherence()\n",
    "\n",
    "    return cov_model, coherence_score\n",
    "\n",
    "cov_model, coherence_score = train_lda(top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc19959c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_max_df = get_doc_topic_matrix(cov_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f02825",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "top_k = 7\n",
    "for t in range(4, top_k):\n",
    "    cov_model = LdaModel(corpus = doc_term_matrix, id2word = dictionary, num_topics = top_k, random_state=66826)\n",
    "\n",
    "    cm = CoherenceModel(model=cov_model, dictionary=dictionary, texts=doc_clean, coherence='c_v')\n",
    "    score = cm.get_coherence()\n",
    "    tup = t, score\n",
    "    results.append(tup)\n",
    "\n",
    "results = pd.DataFrame(results, columns=['topic', 'score'])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef17943",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(results.score.values, index=results.topic.values)\n",
    "_ = s.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12461df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pltk = s.plot()\n",
    "fig = pltk.get_figure()\n",
    "fig.savefig(\"figures/March_topic_k.png\")\n",
    "#pltk.savefig('figures/Jan_topic_k.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2a8119",
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_k = results['score'].max()\n",
    "#results.iloc[results.groupby('topic')['score'].agg(pd.Series.idxmax)]\n",
    "#results.iloc[results.groupby('score').idxmax().values.ravel()]\n",
    "#print(max_k)\n",
    "max_k = results.iloc[results['score'].argmax()][['topic']].astype(int)\n",
    "max_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e31686",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_model = LdaModel(corpus = doc_term_matrix, id2word = dictionary, num_topics = max_k, random_state=66826)\n",
    "\n",
    "cm = CoherenceModel(model=cov_model, dictionary=dictionary, texts=doc_clean, coherence='c_v')\n",
    "coherence = cm.get_coherence()\n",
    "print(coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ff3c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc_topic_max_df = get_doc_topic_matrix(cov_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe221412",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topics = cov_model.get_document_topics(doc_term_matrix, minimum_probability=None, minimum_phi_value=None, per_word_topics=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e346915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_topic_matrix(lda_model):\n",
    "    \"\"\"Caculates the topic probability of each tweet and then assigns the topic with the highest probability.\n",
    "    \n",
    "    Args: lda_model: lda object - trained lda model object.\n",
    "    \n",
    "    Return: \n",
    "        doc_topic_max_df: dataframe - dataframe containing a row for each tweet with a column 'topic_num' indicating the topic with the highest probability for that tweet.\n",
    "    \"\"\"\n",
    "    \n",
    "    doc_topics = lda_model.get_document_topics(doc_term_matrix, minimum_probability=None, minimum_phi_value=None, per_word_topics=False)\n",
    "\n",
    "    doc_topic_max = []\n",
    "\n",
    "    for d in range(len(doc_topics)):\n",
    "        max_topic = max(doc_topics[d])\n",
    "        topic_df = pd.DataFrame(max_topic).transpose()\n",
    "        topic_df.columns = ['topic_num', 'prob']\n",
    "        timeframe_slice = timeframe[['read_text_clean2','Perceived_susceptibility', 'Perceived_severity', 'Perceived_benefits', 'Perceived_barriers']].iloc[[d]]\n",
    "        timeframe_slice = timeframe_slice.reset_index()\n",
    "        topic_df = pd.concat([topic_df, timeframe_slice], axis=1, join=\"inner\")\n",
    "        del topic_df['index'] \n",
    "        doc_topic_max.append(topic_df)\n",
    "        \n",
    "    doc_topic_max_df = pd.concat(doc_topic_max)\n",
    "    \n",
    "    doc_topic_matrix = doc_topic_max_df.groupby(['topic_num'])['Perceived_susceptibility', 'Perceived_severity', 'Perceived_benefits',\n",
    "                                                               'Perceived_barriers'].sum().reset_index()\n",
    "    doc_topic_matrix['count'] = doc_topic_matrix['topic_num'].map(doc_topic_max_df['topic_num'].value_counts())\n",
    "    \n",
    "    return doc_topic_max_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06159b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_max_df = get_doc_topic_matrix(cov_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f69980",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_matrix.to_csv('data/timeframe_results/' + input_date + '_topic_matrix.csv', index = False)\n",
    "doc_topic_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548cb549",
   "metadata": {},
   "source": [
    "# Create a topics table \n",
    "\n",
    "We will have the following columns:\n",
    "\n",
    "* User filter Date\n",
    "* Topic_Num\n",
    "* top terms (string) \n",
    "\n",
    "--Actually --- maybe I just store a few examples of the actual tweets and which topic they had the highest probability of belonging too? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04650c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_max_df_ordered = doc_topic_max_df.sort_values('prob', ascending=False)\n",
    "top_tweets = doc_topic_max_df_ordered.groupby('topic_num').head(3)\n",
    "top_tweets.sort_values('topic_num')\n",
    "\n",
    "top_tweets['date'] = '2020-03-01'\n",
    "top_tweets = top_tweets[['date', 'topic_num', 'prob', 'read_text_clean2', 'Perceived_susceptibility', 'Perceived_severity', 'Perceived_benefits', 'Perceived_barriers']]\n",
    "top_tweets.columns = ['date', 'topic_num', 'prob', 'tweet', 'Perceived_susceptibility', 'Perceived_severity', 'Perceived_benefits', 'Perceived_barriers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490455ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tweets.to_csv('data/top_tweets_march.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e9c55a",
   "metadata": {},
   "source": [
    "# Create Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb01641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=20,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95e2e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = cov_model.show_topics(formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(4,2, figsize=(10,10), sharex=True, sharey=True)\n",
    "\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    axes[3,1].set_axis_off()\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=20))\n",
    "    plt.gca().axis('off')\n",
    "    plt.savefig('figures/March_Model.png', facecolor='w')\n",
    "    plt.gca().imshow(cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6e4ba8",
   "metadata": {},
   "source": [
    "# Save Dataframe to mysql table\n",
    "\n",
    "Save doc_topic_matrix? Or do we save the topics with the top words? \n",
    "\n",
    "Columns: topic 1, word 1, word 2, and number of health beliefs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041c2c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "top_tweets.reset_index()\n",
    "top_tweets['tweet_id'] = np.arange(len(top_tweets))\n",
    "\n",
    "# add this part to not copy over already added tweets\n",
    "top_tweets['tweet_id'] = top_tweets['tweet_id'] + 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c960a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy import Column, Integer, String, MetaData\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from flask_sqlalchemy import SQLAlchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b239bfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Base = declarative_base()\n",
    "\n",
    "class topics(Base):\n",
    "    \"\"\"Create a data model for the database to store topics for time periods.\"\"\"\n",
    "\n",
    "    __tablename__ = 'topics'\n",
    "\n",
    "    read_tweet_id = Column(Integer, primary_key=True)\n",
    "    date = Column(String(10), primary_key=False)\n",
    "    topic_num = Column(Integer, primary_key=False)\n",
    "    prob = Column(Integer, primary_key=False)\n",
    "    tweet = Column(String(300), primary_key=False)\n",
    "    Perceived_susceptibility = Column(Integer, primary_key=False)\n",
    "    Perceived_severity = Column(Integer, primary_key=False)\n",
    "    Perceived_benefits = Column(Integer, primary_key=False)\n",
    "    Perceived_barriers = Column(Integer, primary_key=False)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '<topics %r>' % self.tweet\n",
    "\n",
    "def create_db(engine_string: str):\n",
    "    \"\"\"Create database from provided engine string.\n",
    "\n",
    "    Args:\n",
    "        engine_string: str - Engine string.\n",
    "\n",
    "    Returns: \n",
    "        engine: sqlalchemy.engine.base.Engine - sqlalchemy connection from amazon rds  \n",
    "\n",
    "    \"\"\"\n",
    "    #logger.debug(\"Create table and columns for raw data.\")\n",
    "    engine = sqlalchemy.create_engine(engine_string)\n",
    "\n",
    "    Base.metadata.create_all(engine)\n",
    "    \n",
    "    return engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72409c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_type = \"mysql+pymysql\"\n",
    "user=\"admin\"\n",
    "password=\"PASSWORD HERE\"\n",
    "host=\"HOST HERE\" \n",
    "port=\"3306\"\n",
    "db_name=\"msia423_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d890686",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, MetaData, Table, Column, Integer, String\n",
    "\n",
    "engine_string = f\"{conn_type}://{user}:{password}@{host}:{port}/{db_name}\"\n",
    "\n",
    "engine = create_db(engine_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80cf855",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tweets.to_sql(name='topics', con=engine, if_exists = 'append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d7c539",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, MetaData, Table, Column, Integer, String\n",
    "\n",
    "engine_string = f\"{conn_type}://{user}:{password}@{host}:{port}/{db_name}\"\n",
    "\n",
    "engine = create_engine(engine_string, echo = True)\n",
    "meta = MetaData()\n",
    "\n",
    "students = Table(\n",
    "   'students', meta, \n",
    "   Column('id', Integer, primary_key = True)\n",
    ")\n",
    "meta.create_all(engine)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
